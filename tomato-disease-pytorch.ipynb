{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5441978,"sourceType":"datasetVersion","datasetId":3146821}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torchvision import models, transforms\nfrom sklearn import svm\nimport torchinfo\n\nfrom PIL import Image\n\n# 1. Load the pre-trained EfficientNetB0 model\nmodel = models.efficientnet_b0(pretrained=True)\n\nmodel.eval()\n\n# 3. Define a transform for image preprocessing (adjust as needed)\ntransform = transforms.Compose([\n    transforms.Resize(224),  # Resize images to EfficientNetB0 input size\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize with ImageNet statistics\n])\n\n# 4. Function to extract features from an image\ndef extract_features(image_path):\n    image = transform(Image.open(image_path)).unsqueeze(0)  # Add a batch dimension\n    with torch.no_grad():\n        features = model.features(image)  # Extract features from convolutional layers\n        pooled_features = torch.nn.functional.adaptive_avg_pool2d(features, output_size=(1, 1)).squeeze()\n        return pooled_features\n\n# 5. Extract features for your dataset\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-24T14:09:34.267879Z","iopub.execute_input":"2024-03-24T14:09:34.268239Z","iopub.status.idle":"2024-03-24T14:09:34.448595Z","shell.execute_reply.started":"2024-03-24T14:09:34.268210Z","shell.execute_reply":"2024-03-24T14:09:34.447651Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"}]},{"cell_type":"code","source":"from tqdm import tqdm\nimport numpy as np\nx_train = []\ny_train = []\nlabels = {'Bacterial Spot':0,'Early Blight':1,'Healthy':2,'Late Blight':3,'Septoria Leaf Spot':4,'Yellow Leaf Curl Virus':5}\nfor i in labels:\n    folderPath = os.path.join('/kaggle/input/plant-village-dataset-updated/Tomato/Train',i)\n    for j in tqdm(os.listdir(folderPath)):\n        if j.lower().endswith((\".jpg\", \".png\", \".jpeg\")):\n            features=extract_features(os.path.join(folderPath,j))\n            x_train.append(features)\n            y_train.append(labels[i])\n        \nfor i in labels:\n    folderPath = os.path.join('/kaggle/input/plant-village-dataset-updated/Tomato/Test',i)\n    for j in tqdm(os.listdir(folderPath)):\n        if j.lower().endswith((\".jpg\", \".png\", \".jpeg\")):\n            features=extract_features(os.path.join(folderPath,j))\n\n            x_train.append(features)\n            y_train.append(labels[i])\n        \nx_train = np.array(x_train)\ny_train = np.array(y_train)\nprint(x_train.shape,y_train.shape)\n\nx_train,x_test,y_train,y_test=train_test_split(x_train, y_train, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-03-24T14:09:41.874687Z","iopub.execute_input":"2024-03-24T14:09:41.875081Z","iopub.status.idle":"2024-03-24T14:18:09.797533Z","shell.execute_reply.started":"2024-03-24T14:09:41.875042Z","shell.execute_reply":"2024-03-24T14:18:09.796331Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"100%|██████████| 1702/1702 [01:17<00:00, 21.98it/s]\n100%|██████████| 1920/1920 [01:25<00:00, 22.52it/s]\n100%|██████████| 1926/1926 [01:24<00:00, 22.80it/s]\n100%|██████████| 1851/1851 [01:21<00:00, 22.66it/s]\n100%|██████████| 1746/1746 [01:20<00:00, 21.80it/s]\n100%|██████████| 1961/1961 [01:26<00:00, 22.76it/s]\n100%|██████████| 43/43 [00:02<00:00, 20.37it/s]\n100%|██████████| 48/48 [00:02<00:00, 22.65it/s]\n100%|██████████| 49/49 [00:02<00:00, 22.55it/s]\n100%|██████████| 47/47 [00:02<00:00, 22.47it/s]\n100%|██████████| 44/44 [00:01<00:00, 22.17it/s]\n100%|██████████| 49/49 [00:02<00:00, 23.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"(11385, 1280) (11385,)\n","output_type":"stream"}]},{"cell_type":"code","source":"clf = svm.SVC()\nclf.fit(x_train,y_train)\njoblib.dump(clf, 'svm_tomato_pytorch.pkl')\ny_pred=clf.predict(x_test)\naccuracy_deep = accuracy_score(y_test,y_pred)\nC = confusion_matrix(y_test,y_pred)\n\nprint(\"Accuracy:\", accuracy_deep)\nprint(\"Confusion Matrix:\")\nprint(C)\n\n# 7. Use the trained SVM to classify new images\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-24T12:58:16.418695Z","iopub.execute_input":"2024-03-24T12:58:16.419366Z","iopub.status.idle":"2024-03-24T12:58:37.524804Z","shell.execute_reply.started":"2024-03-24T12:58:16.419323Z","shell.execute_reply":"2024-03-24T12:58:37.523744Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Accuracy: 0.9819938515590689\nConfusion Matrix:\n[[361   1   0   2   0   0]\n [  4 384   1   2   7   2]\n [  0   1 385   0   0   0]\n [  0   5   1 367   2   0]\n [  3   5   0   0 356   1]\n [  2   1   1   0   0 383]]\n","output_type":"stream"}]},{"cell_type":"code","source":"with open('/kaggle/working/svm_tomato_pytorch.pkl', 'rb') as f:\n    svm_classifier = joblib.load(f, mmap_mode=None)\ny_pred2=svm_classifier.predict(x_test)\naccuracy_deep = accuracy_score(y_test,y_pred2)\nC = confusion_matrix(y_test,y_pred2)\n\nprint(\"Accuracy:\", accuracy_deep)\nprint(\"Confusion Matrix:\")\nprint(C)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-24T13:20:22.529669Z","iopub.execute_input":"2024-03-24T13:20:22.530300Z","iopub.status.idle":"2024-03-24T13:20:30.597102Z","shell.execute_reply.started":"2024-03-24T13:20:22.530266Z","shell.execute_reply":"2024-03-24T13:20:30.596074Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Accuracy: 0.9819938515590689\nConfusion Matrix:\n[[361   1   0   2   0   0]\n [  4 384   1   2   7   2]\n [  0   1 385   0   0   0]\n [  0   5   1 367   2   0]\n [  3   5   0   0 356   1]\n [  2   1   1   0   0 383]]\n","output_type":"stream"}]}]}